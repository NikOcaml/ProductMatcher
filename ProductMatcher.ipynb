{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Initialization Cell",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "ProductMatcher.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-01-29T11:25:39.961903Z",
          "start_time": "2021-01-29T11:25:32.210134Z"
        },
        "init_cell": true,
        "id": "fbblu69eGA7H"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from skimage.transform import resize\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import statistics\n",
        "import matplotlib.pyplot as plt;\n",
        "from google.colab import files\n",
        "from sklearn import metrics\n",
        "import seaborn as sb"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-01-29T11:25:52.897632Z",
          "start_time": "2021-01-29T11:25:40.103402Z"
        },
        "id": "xi93LVfMGA7O"
      },
      "source": [
        "embedding = hub.KerasLayer(\n",
        "    \"https://tfhub.dev/google/nnlm-de-dim128-with-normalization/2\", \n",
        "    input_shape=[],\n",
        "    dtype=tf.string)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxnh9ZDSAPCN",
        "outputId": "451b9496-3cf8-480e-e461-8f3ea57a01c1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-01-29T11:25:39.968898Z",
          "start_time": "2021-01-29T11:25:39.963900Z"
        },
        "id": "h1YiYdorGA7K"
      },
      "source": [
        "#Der Header funkt\n",
        "HEADERS = {'User-Agent':\n",
        "            'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36',\n",
        "            'Accept-Language': 'de,pl;q=0.9,cs;q=0.8,en;q=0.7,ru;q=0.6,ko;q=0.5'}"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yslMOzoGA7M"
      },
      "source": [
        "2 Verschiedene \"Modi\":\n",
        "- 1  Automatisches Erstellen der Listen anhand von Keyword und Referenzprodukt\n",
        "- 2 Bearbeitung (Score assessment) existierender CSVs (Pfad ggf. einfach ändern)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-01-29T11:25:39.987887Z",
          "start_time": "2021-01-29T11:25:39.972895Z"
        },
        "id": "TrVEr0yyGA7M"
      },
      "source": [
        "Modi=2 #1 oder 2\n",
        "PATH=\"/content/drive/MyDrive/contrastive_dataset (1).csv\""
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-01-29T11:25:40.067443Z",
          "start_time": "2021-01-29T11:25:39.997881Z"
        },
        "id": "s_t6NrIQGA7N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a26aa5fc-1872-42e9-ca97-2d80a577c774"
      },
      "source": [
        "#1:\n",
        "def Get_items(soups):\n",
        "  urls=[]\n",
        "  Titles=[]\n",
        "  ASINs=[]\n",
        "  for soup in soups:\n",
        "    for Prod_q in soup.select(\"a.a-link-normal.a-text-normal\"):\n",
        "        Prod=Prod_q.select('span.a-size-base-plus.a-color-base.a-text-normal')\n",
        "        if Prod!=[]:\n",
        "            Titles.append(Prod[0].get_text())\n",
        "            hreff=re.search(\"(?<=dp).*\", Prod_q.find_parent().find(name=\"a\")[\"href\"]).group(0)\n",
        "            if hreff[0]==\"%\":\n",
        "                ASINs.append(hreff.split(\"%\")[1][2:])\n",
        "            else:\n",
        "                ASINs.append(hreff.split(\"/\")[1])\n",
        "  return Titles, ASINs\n",
        "if Modi==1:\n",
        "    #Keyword (Manuell)\n",
        "    Keywords=[\"Haarschere\"]\n",
        "    #Titel des Ref. Produkts (Manuell)\n",
        "    Ref_Prod=\"Haarschere Premium Friseurscheren, Lidasen Scharfe Licht Ausdünnen Haarschneideschere, Edelstahl Friseurschere Profi Frisörschere mit Etui, Perfekter Effilierschere für Damen und Herren\"\n",
        "    #Soups der Result pages\n",
        "    Result_soups = [BeautifulSoup(requests.get(\"https://www.amazon.de/s?k=\"+keyw.replace(\" \", \"+\"), headers=HEADERS).text, \"html5lib\") for keyw in Keywords]\n",
        "    #Gibt Listen der ASINs und Titel der Produkte wieder  \n",
        "    #Listen der Titel und ASINs \n",
        "    Titles, ASINs=Get_items(Result_soups)\n",
        "#2:\n",
        "elif Modi==2:\n",
        "    Dataset=pd.read_csv(PATH, index_col=\"ASIN\")\n",
        "    ASINs=list(Dataset.index)\n",
        "    #Ref_Prod=list(Dataset[Dataset.Comment==\"Referenz\"].Title)\n",
        "    Prod_Names=Dataset[\"Product name\"].unique()\n",
        "    Ref_Prod=list(Dataset[Dataset.Comment==\"Referenz\"].Title)\n",
        "    print(Prod_Names)\n",
        "    Product={}\n",
        "    Refs={}\n",
        "    for i in range(len(Prod_Names)):\n",
        "      Product[Prod_Names[i]]=Dataset[Dataset[\"Product name\"]==Prod_Names[i]]\n",
        "      Refs[Prod_Names[i]]=Ref_Prod[i]\n",
        "urls=[\"https://www.amazon.de/dp/\"+ ASIN for ASIN in ASINs]"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Aktenvernichter' 'Bürolocher' 'Effilierschere' 'Fusselrasierer'\n",
            " 'Haarschere' 'handy stativ' 'Laptop Ständer' 'Smoothie Maker']\n",
            "dict_keys(['Aktenvernichter', 'Bürolocher', 'Effilierschere', 'Fusselrasierer', 'Haarschere', 'handy stativ', 'Laptop Ständer', 'Smoothie Maker'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eV5pfjcPt0W8"
      },
      "source": [
        "Cleaning als auch Style/Farbe/Material Zuordnung."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-01-29T11:25:40.100403Z",
          "start_time": "2021-01-29T11:25:40.069422Z"
        },
        "id": "Y8oTd2cXGA7N"
      },
      "source": [
        "#Provisorische Blacklist. Elemente aus dieser Liste werden aus den Titeln entfernt.\n",
        "Clean_Prov=[\",\", \"/\", \"(\", \")\", \"|\", \".\", \"-\"]\n",
        "\n",
        "#Entfernt Strings die in \"Clean_prov\" enthalten sind aus den Titeln\n",
        "#Und Gibt Listen der übrigen Wörter wieder: [[\"Titel(,) Test\"], [\"Test|2\"]] -> [[\"Titel\", \"Test\"], [\"Test\", \"2\"]]\n",
        "def Clean_WordLister(Title, Clean=Clean_Prov):\n",
        "    for elem in Clean:\n",
        "        Title=Title.replace(elem, \"\")\n",
        "    #return list(dict.fromkeys(Title.lower().split(\" \")))\n",
        "    return list(filter(None, list(dict.fromkeys(Title.lower().split(\" \")))))\n",
        "#S.o.\n",
        "def Clean_SentenceLister(Title, Clean=Clean_Prov):\n",
        "  for elem in Clean:\n",
        "    Title=Title.replace(elem, \"\")\n",
        "  return Title.lower()\n",
        "#Returned vorg. Eigenschaften in Key_Elements. Input am Besten von CleanSentenceLister.\n",
        "Key_Elements=[\"farbe\", \"Stil\", \"länge\", \"größe\"]\n",
        "def Preprocess_Title(Title):\n",
        "  matched_elements={}\n",
        "  for q in Key_Elements:\n",
        "    a=list(filter(None, re.search('(?<='+q+').*', Title).group().split(\" \")))\n",
        "    if len(a)!=0:\n",
        "      matched_elements[q]=a[0]"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwsPPllStgym"
      },
      "source": [
        "Metriken und dessen Funktionen zur Berechnung der Abstände der Embeddings (Wort- als auch Satzvektoren)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njzDreSNr64Y"
      },
      "source": [
        "#Bildung der Wortvektoren anhand der Clean_Titles_W\r\n",
        "#Embeddings werden in einem Dictionary gespeichert und falls existent wird\r\n",
        "#Dieses abgerufen anstatt embedding nochmal zu callen.\r\n",
        "def Embedd_Titles_W(Title_List):\r\n",
        "    Embeddeds={}\r\n",
        "    Ordered_Emb=[]\r\n",
        "    for i in range(len(Title_List)):\r\n",
        "        Ordered_Emb.append([])\r\n",
        "        for j in Title_List[i]:\r\n",
        "          if j not in Embeddeds.keys():\r\n",
        "            Embeddeds[j]=embedding([j])\r\n",
        "          Ordered_Emb[i].append(Embeddeds[j])\r\n",
        "    return Ordered_Emb\r\n",
        "def Euclid(Embedd1, Embedd2):\r\n",
        "    return np.linalg.norm(Embedd1-Embedd2)\r\n",
        "def Manhattan(Embedd1, Embedd2):\r\n",
        "    return np.sum(np.abs(Embedd1-Embedd2))\r\n",
        "def Maximums(Embedd1, Embedd2):\r\n",
        "    return np.max(np.abs(Embedd1-Embedd2))\r\n",
        "def Cos_Sim(Embedd1, Embedd2):\r\n",
        "    return np.array((Embedd1@np.array(Embedd2).T)/(np.linalg.norm(Embedd1)*np.linalg.norm(Embedd2)))\r\n",
        "#Für jedes Wort im Titel des Referenzprodukts das im zum vergleichenden Produkttitel vorkommt\r\n",
        "#+1 Score, Geteilt durch die Anzahl der Wörter, d.h. max. Score = 1\r\n",
        "def EasyScore(titles, ref):\r\n",
        "    Scores=np.zeros((len(titles)))\r\n",
        "    for elem in ref:\r\n",
        "        for i in range(len(titles)):\r\n",
        "            for j in range(len(titles[i])):\r\n",
        "                if elem==titles[i][j]:\r\n",
        "                    Scores[i]+=1\r\n",
        "    return [Scores[i]/len(titles[i]) for i in range(len(titles))]\r\n",
        "#Wortvektoren\r\n",
        "def Vec_Score_W(Ordered_Emb, Clean_Ref):\r\n",
        "    Scores_Euclid=[]\r\n",
        "    Scores_Manhattan=[]\r\n",
        "    Scores_Maximums=[]\r\n",
        "    Scores_Cos=[]\r\n",
        "    for emb in Ordered_Emb:\r\n",
        "        Temp_Scores=[[],[],[],[]]\r\n",
        "        for elem in emb:\r\n",
        "          for ref in Clean_Ref:\r\n",
        "              Temp_Scores[0].append(Euclid(elem, ref))\r\n",
        "              Temp_Scores[1].append(Manhattan(elem, ref))\r\n",
        "              Temp_Scores[2].append(Maximums(elem, ref))\r\n",
        "              Temp_Scores[3].append(Cos_Sim(elem, ref))\r\n",
        "        Scores_Euclid.append(np.sum(Temp_Scores[0])/len(emb))\r\n",
        "        Scores_Manhattan.append(np.sum(Temp_Scores[1])/len(emb))\r\n",
        "        Scores_Maximums.append(np.sum(Temp_Scores[2])/len(emb))\r\n",
        "        Scores_Cos.append(np.sum(Temp_Scores[3])/len(emb))\r\n",
        "    return Scores_Euclid, Scores_Manhattan, Scores_Maximums, Scores_Cos\r\n",
        "#Satzvektoren\r\n",
        "def Vec_Score_S(Ordered_Emb, Clean_Ref):\r\n",
        "    Scores_Euclid=[]\r\n",
        "    Scores_Manhattan=[]\r\n",
        "    Scores_Maximums=[]\r\n",
        "    Scores_Cos=[]\r\n",
        "    for elem in Ordered_Emb:\r\n",
        "      Scores_Euclid.append(Euclid(elem, Clean_Ref))\r\n",
        "      Scores_Manhattan.append(Manhattan(elem, Clean_Ref))\r\n",
        "      Scores_Maximums.append(Maximums(elem, Clean_Ref))\r\n",
        "      Scores_Cos.append(Cos_Sim(elem, Clean_Ref))\r\n",
        "    return Scores_Euclid, Scores_Manhattan, Scores_Maximums, Scores_Cos"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6B-H0OSSs-vq"
      },
      "source": [
        "Fkt zur Standardiesierung der Scores: $MinMax$, $Z-Score$ oder $1/Max$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJp_jeQdhq8S"
      },
      "source": [
        "#Modes: \"MinMax\", \"Z-Score\", \"1/Max\"\r\n",
        "def Standardization(Score, Mode=\"Z-Score\"):\r\n",
        "  Score=np.array(Score)\r\n",
        "  a=len(Score)\r\n",
        "  Stand_Score=np.zeros(a)\r\n",
        "  if Mode==\"MinMax\":\r\n",
        "    Stand_Score=(Score-min(Score))*1/(max(Score)-min(Score))\r\n",
        "  elif Mode==\"Z-Score\":\r\n",
        "    Stand_Score=np.abs((Score-np.mean(Score))*1/np.std(Score))\r\n",
        "  elif Mode==\"1/Max\":\r\n",
        "    Stand_Score=np.abs(Score*(1/(max(Score))))\r\n",
        "  return Stand_Score"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iRFz7r5PMak"
      },
      "source": [
        "Kombinierte \"Master\" Fkt mit Input: (Produkt Name, Opt.: Referenzprodukt, Opt.: Modus) -> Output: (Stand. Scores)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQXptg_QPMBx"
      },
      "source": [
        "def Access_Scores(prod, ref=0, Modus=\"1/Max\"):\r\n",
        "  Prod=Product[prod]\r\n",
        "  if ref==0:\r\n",
        "    Ref=Refs[prod]\r\n",
        "  else:\r\n",
        "    Ref=Refs[ref]\r\n",
        "  Clean_Ref_W=Clean_WordLister(Ref)\r\n",
        "  Clean_Ref_S=Clean_SentenceLister(Ref)\r\n",
        "  Clean_Titles_W=[]\r\n",
        "  Clean_Titles_S=[]\r\n",
        "  for elem in Prod.Title:\r\n",
        "    Clean_Titles_W.append(Clean_WordLister(elem))\r\n",
        "    Clean_Titles_S.append(Clean_SentenceLister(elem))\r\n",
        "  Emb_W=Embedd_Titles_W(Clean_Titles_W)\r\n",
        "  Emb_S=[embedding([i]) for i in Clean_Titles_S]\r\n",
        "  Clean_Ref_Emb_W=[embedding([i]) for i in Clean_Ref_W]\r\n",
        "  Clean_Ref_Emb_S=embedding([Clean_Ref_S])\r\n",
        "  Euc_W, Man_W, Maxi_W, Cos_W = Vec_Score_W(Emb_W, Clean_Ref_Emb_W)\r\n",
        "  Euc_S, Man_S, Maxi_S, Cos_S=Vec_Score_S(Emb_S, Clean_Ref_Emb_S)\r\n",
        "  #Cos_S=[-Cos_S[i][0][0] for i in range(len(Cos_S))]\r\n",
        "  W_Scores=[Euc_W, Man_W, Maxi_W, Cos_W]\r\n",
        "  S_Scores=[Euc_S, Man_S, Maxi_S, Cos_S]\r\n",
        "  W_Scores_Norm=[Standardization(i, Mode=Modus) for i in W_Scores]\r\n",
        "  S_Scores_Norm=[Standardization(i, Mode=Modus) for i in S_Scores]\r\n",
        "  #S_Scores_Norm[-1]=Standardization(Cos_S, Mode=Modus)\r\n",
        "  return W_Scores_Norm, S_Scores_Norm"
      ],
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TFRWe6ImJyB"
      },
      "source": [
        "#Output: [Max_Accuracy, Best_Treshold]\r\n",
        "def Treshold_Opt(Score, Prod):\r\n",
        "  Classes=Product[Prod]\r\n",
        "  Classes=Classes[\"Product class\"].tolist()\r\n",
        "  med=np.median(Score)\r\n",
        "  minn=np.min(Score)\r\n",
        "  Tresh=0.5*(med+minn)\r\n",
        "  Max_Tresh=np.zeros(2)\r\n",
        "  while Tresh < 0.999:\r\n",
        "    Class_Pred=[0 if i < Tresh else 1 for i in Score]\r\n",
        "    temp=metrics.accuracy_score(Classes, Class_Pred)\r\n",
        "    if temp>Max_Tresh[0]:\r\n",
        "      Max_Tresh[0]=temp\r\n",
        "      Max_Tresh[1]=Tresh\r\n",
        "    Tresh+=0.0001\r\n",
        "  return Max_Tresh"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lk4X6hp8eAEa",
        "outputId": "5ca91e93-4256-4c97-8327-6da4ed982283"
      },
      "source": [
        "print(\"Format: [Max. Accuracy, Best Treshold]\")\r\n",
        "for i in Prod_Names:\r\n",
        "  Word, Sent= Access_Scores(i)\r\n",
        "  print(\"Accuracy der Wordscores & Treshold für \"+i+\":\")\r\n",
        "  for j in Word:\r\n",
        "    print(Treshold_Opt(j, i))\r\n",
        "  print(\"Accuracy der Sentencescores & Treshold für \"+i+\":\")\r\n",
        "  for k in Sent:\r\n",
        "    print(Treshold_Opt(k, i))"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Format: [Max. Accuracy, Best Treshold]\n",
            "Wordscores & Treshold für Aktenvernichter:\n",
            "[0.88235294 0.99293162]\n",
            "[0.88235294 0.99041455]\n",
            "[0.82352941 0.99017359]\n",
            "[0.82352941 0.49060635]\n",
            "Sentencescores & Treshold für Aktenvernichter:\n",
            "[0.82352941 0.99442801]\n",
            "[0.82352941 0.99193702]\n",
            "[0.82352941 0.92183948]\n",
            "[0.82352941 0.64894876]\n",
            "Wordscores & Treshold für Bürolocher:\n",
            "[0.87878788 0.97432461]\n",
            "[0.87878788 0.97414171]\n",
            "[0.87878788 0.96025556]\n",
            "[0.75757576 0.93190479]\n",
            "Sentencescores & Treshold für Bürolocher:\n",
            "[0.84848485 0.75166712]\n",
            "[0.87878788 0.79135567]\n",
            "[0.84848485 0.75363801]\n",
            "[0.75757576 0.85933751]\n",
            "Wordscores & Treshold für Effilierschere:\n",
            "[0.52631579 0.94688786]\n",
            "[0.52631579 0.94816207]\n",
            "[0.52631579 0.93189973]\n",
            "[0.63157895 0.47583786]\n",
            "Sentencescores & Treshold für Effilierschere:\n",
            "[0.54385965 0.73888925]\n",
            "[0.54385965 0.65805762]\n",
            "[0.59649123 0.79235632]\n",
            "[0.61403509 0.38997995]\n",
            "Wordscores & Treshold für Fusselrasierer:\n",
            "[0.75757576 0.95144083]\n",
            "[0.74242424 0.94639357]\n",
            "[0.8030303 0.939765 ]\n",
            "[0.63636364 0.92843933]\n",
            "Sentencescores & Treshold für Fusselrasierer:\n",
            "[0.66666667 0.7323543 ]\n",
            "[0.65151515 0.72414798]\n",
            "[0.66666667 0.97456901]\n",
            "[0.63636364 0.77217574]\n",
            "Wordscores & Treshold für Haarschere:\n",
            "[0.63157895 0.98967222]\n",
            "[0.63157895 0.99261095]\n",
            "[0.63157895 0.99847502]\n",
            "[0.77192982 0.68794221]\n",
            "Sentencescores & Treshold für Haarschere:\n",
            "[0.63157895 0.98058478]\n",
            "[0.63157895 0.96673143]\n",
            "[0.61403509 0.871109  ]\n",
            "[0.75438596 0.79598145]\n",
            "Wordscores & Treshold für handy stativ:\n",
            "[0.93023256 0.99708397]\n",
            "[0.93023256 0.9973023 ]\n",
            "[0.90697674 0.99695801]\n",
            "[0.93023256 0.98319525]\n",
            "Sentencescores & Treshold für handy stativ:\n",
            "[0.93023256 0.99538627]\n",
            "[0.90697674 0.97915036]\n",
            "[0.93023256 0.99594647]\n",
            "[0.93023256 0.88897041]\n",
            "Wordscores & Treshold für Laptop Ständer:\n",
            "[0.9        0.99646728]\n",
            "[0.9        0.99875847]\n",
            "[0.9        0.99667136]\n",
            "[0.90833333 0.97000332]\n",
            "Sentencescores & Treshold für Laptop Ständer:\n",
            "[0.90833333 0.98613873]\n",
            "[0.90833333 0.97928046]\n",
            "[0.90833333 0.91556686]\n",
            "[0.90833333 0.92178819]\n",
            "Wordscores & Treshold für Smoothie Maker:\n",
            "[0.92424242 0.9945507 ]\n",
            "[0.92424242 0.99636177]\n",
            "[0.93939394 0.99278943]\n",
            "[0.93939394 0.90504389]\n",
            "Sentencescores & Treshold für Smoothie Maker:\n",
            "[0.93939394 0.89362851]\n",
            "[0.93939394 0.93130882]\n",
            "[0.93939394 0.94257806]\n",
            "[0.93939394 0.80154796]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnAsqXKQB4x-"
      },
      "source": [
        "'''%matplotlib inline\r\n",
        "sb.clustermap(Dataset.iloc[:,-12:-8])'''"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}